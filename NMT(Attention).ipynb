{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NMT(Attention).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOfailxJbsVsRvwgXl9vM9g",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ysj9909/NLP_practice/blob/main/NMT(Attention).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE (ICLR 2015) 코드 구현 연습\n"
      ],
      "metadata": {
        "id": "89bSK_Y4mD7I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 데이터 전처리\n",
        " * spacy Library를 이용해서 영어, 독일어 전처리 진행"
      ],
      "metadata": {
        "id": "i1aSfnoGJTho"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "E_NgpsAAFmPi"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!python -m spacy download en\n",
        "!python -m spacy download de"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "spacy_en = spacy.load(\"en\")   # 영어 토큰화\n",
        "spacy_de = spacy.load(\"de\")   # 독일어 토큰화"
      ],
      "metadata": {
        "id": "uR-GwmbNKyu3"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 토크나이저 실습\n",
        "text = \"I'm eating instant noodles\"\n",
        "\n",
        "for i, word in enumerate([token.text for token in spacy_en.tokenizer(text)]):\n",
        "  print(f\"{i + 1} 번째 토큰은 {word}입니다.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bh63S9BiMDpW",
        "outputId": "e87a9286-a2bd-487b-9956-7d57fcb49662"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 번째 토큰은 I입니다.\n",
            "2 번째 토큰은 'm입니다.\n",
            "3 번째 토큰은 eating입니다.\n",
            "4 번째 토큰은 instant입니다.\n",
            "5 번째 토큰은 noodles입니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 토크나이저 함수 정의\n",
        "def tokenize_en(text):\n",
        "  # 영어 문장 스트링으로부터 단어 스트링들의 리스트로 변환\n",
        "  return [token.text for token in spacy_en.tokenizer(text)]\n",
        "\n",
        "def tokenize_de(text):\n",
        "  # 독일어 문장 스트링으로부터 단어 스트링들의 리스트로 변환하고 성능 향상을 위해 순서를 반대로 바꿈\n",
        "  return [token.text for token in spacy_de.tokenizer(text)][::-1]"
      ],
      "metadata": {
        "id": "tKM9ky1aObI0"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 독일어 - 영어 번역 데이터셋을 불러들인다\n",
        "from torchtext.legacy.datasets import Multi30k\n",
        "# Field를 통해 전처리 명명, 필요한 패드 수를 최소화할 수 있는 배치(Batch) Iterator를 불러온다\n",
        "from torchtext.legacy.data import Field, BucketIterator\n",
        "\n",
        "SRC = Field(tokenize = tokenize_de,\n",
        "            init_token = \"<sos>\",\n",
        "            eos_token = \"<eos>\",\n",
        "            lower = True)\n",
        "TRG = Field(tokenize = tokenize_en, \n",
        "            init_token = \"<sos>\",\n",
        "            eos_token = \"<eos>\",\n",
        "            lower = True)\n",
        "\n",
        "train_dataset , valid_dataset, test_dataset = Multi30k.splits(exts = (\".de\", \".en\"),\n",
        "                                                              fields = (SRC, TRG))"
      ],
      "metadata": {
        "id": "de_KKwwORjmD"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터의 개수 확인\n",
        "print(f\"Number of training examples : {len(train_dataset.examples)}\")\n",
        "print(f\"Number of validation examples : {len(valid_dataset.examples)}\")\n",
        "print(f\"Number of test examples : {len(test_dataset.examples)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n8AyAzFBY7rh",
        "outputId": "30d17ce7-c667-437b-d7dd-ee6dc85f8965"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training examples : 29000\n",
            "Number of validation examples : 1014\n",
            "Number of test examples : 1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터의 타입 확인(src data가 순서가 뒤집어져 있는 것을 확인할 수 있다.)\n",
        "print(vars(train_dataset.examples[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3iMEKW8PZzQq",
        "outputId": "831b0a0c-48a8-4969-857e-7a6a78e8109c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'src': ['.', 'büsche', 'vieler', 'nähe', 'der', 'in', 'freien', 'im', 'sind', 'männer', 'weiße', 'junge', 'zwei'], 'trg': ['two', 'young', ',', 'white', 'males', 'are', 'outside', 'near', 'many', 'bushes', '.']}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Vocabulary 객체 생성\n",
        "SRC.build_vocab(train_dataset, min_freq = 2)\n",
        "TRG.build_vocab(train_dataset, min_freq = 2)\n",
        "\n",
        "print(f\"length of source Vocabulary : {len(SRC.vocab)}\")\n",
        "print(f\"length of target Vocabulary : {len(TRG.vocab)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7lfoBNYsaIps",
        "outputId": "ff8e3edd-eb82-4d3a-b358-41a196fd380f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of source Vocabulary : 7855\n",
            "length of target Vocabulary : 5893\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import random\n",
        "import math\n",
        "import time\n",
        "\n",
        "# device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "eve9c7JBbhfl"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyper-parameters\n",
        "batch_size = 128\n",
        "num_epochs = 10\n",
        "learning_rate = 0.001\n",
        "hidden_dim = 1000\n",
        "embed_dim = 620\n",
        "att_dim = 1000"
      ],
      "metadata": {
        "id": "qtXOepVUcFuB"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterator 생성\n",
        "trainloader, validloader, testloader = BucketIterator.splits(datasets = (train_dataset, valid_dataset, test_dataset),\n",
        "                                                             batch_size = batch_size,\n",
        "                                                             device = device)"
      ],
      "metadata": {
        "id": "n_hyOpcndFwl"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(TRG.vocab[\"<sos>\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mu16pbidfPep",
        "outputId": "61dbca99-0608-4917-b879-0e96c3239e92"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 배치(batch) 형태 확인\n",
        "for batch in trainloader:\n",
        "  print(batch.src.size())\n",
        "  print(batch.trg.size())\n",
        "\n",
        "  trg_text = batch.trg[:, 0]\n",
        "  for i, token in enumerate(trg_text):\n",
        "    print(f\"index : {i}, token : {token}\")\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2389tO9WhTC5",
        "outputId": "e052bb76-51c7-4102-8290-252942214417"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 128])\n",
            "torch.Size([31, 128])\n",
            "index : 0, token : 2\n",
            "index : 1, token : 33\n",
            "index : 2, token : 6\n",
            "index : 3, token : 256\n",
            "index : 4, token : 162\n",
            "index : 5, token : 5001\n",
            "index : 6, token : 44\n",
            "index : 7, token : 97\n",
            "index : 8, token : 182\n",
            "index : 9, token : 76\n",
            "index : 10, token : 44\n",
            "index : 11, token : 172\n",
            "index : 12, token : 5\n",
            "index : 13, token : 3\n",
            "index : 14, token : 1\n",
            "index : 15, token : 1\n",
            "index : 16, token : 1\n",
            "index : 17, token : 1\n",
            "index : 18, token : 1\n",
            "index : 19, token : 1\n",
            "index : 20, token : 1\n",
            "index : 21, token : 1\n",
            "index : 22, token : 1\n",
            "index : 23, token : 1\n",
            "index : 24, token : 1\n",
            "index : 25, token : 1\n",
            "index : 26, token : 1\n",
            "index : 27, token : 1\n",
            "index : 28, token : 1\n",
            "index : 29, token : 1\n",
            "index : 30, token : 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 모델 정의"
      ],
      "metadata": {
        "id": "K5SQntcmddyJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LSTM을 사용\n",
        "class Encoder(nn.Module):\n",
        "  def __init__(self, vocab_size, embed_dim, hidden_dim, n_layers =2):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.n_layers = n_layers\n",
        "    self.hidden_dim = hidden_dim\n",
        "\n",
        "    self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "\n",
        "    self.Bi_LSTM = nn.LSTM(embed_dim, hidden_dim, num_layers = n_layers, batch_first = True, bidirectional = True, dropout = 0.5)\n",
        "\n",
        "  def forward(self, src):\n",
        "    # src : source sentences , shape of (src_length, batch_size)\n",
        "    src = src.permute(1, 0)   # (batch_size, src_length)\n",
        "    emb = self.embedding(src)  # (batch_size, src_length, embed_dim)\n",
        "    \n",
        "    h0 = torch.zeros(2 * self.n_layers, src.size(0), self.hidden_dim).to(device)\n",
        "    c0 = torch.zeros(2 * self.n_layers, src.size(0), self.hidden_dim).to(device)\n",
        "\n",
        "    output, _ = self.Bi_LSTM(emb, (h0, c0))    # (batch_size, src_length, 2 * hidden_dim)\n",
        "\n",
        "    return output"
      ],
      "metadata": {
        "id": "QwmG3F6BdcWI"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 논문에서의 구조를 그대로 구현함(GRU 구현)\n",
        "class Decoder(nn.Module):\n",
        "  def __init__(self, vocab_size, embed_dim, hidden_dim, att_dim):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.vocab_size = vocab_size\n",
        "    self.embed_dim = embed_dim\n",
        "    self.hidden_dim = hidden_dim\n",
        "    \n",
        "    self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "\n",
        "    self.W = nn.Linear(embed_dim, hidden_dim)\n",
        "    self.U = nn.Linear(hidden_dim, hidden_dim)\n",
        "    self.C = nn.Linear(2 * hidden_dim, hidden_dim)\n",
        "    self.W_z = nn.Linear(embed_dim, hidden_dim)\n",
        "    self.U_z = nn.Linear(hidden_dim, hidden_dim)\n",
        "    self.C_z = nn.Linear(2 * hidden_dim, hidden_dim)\n",
        "    self.W_r = nn.Linear(embed_dim, hidden_dim)\n",
        "    self.U_r = nn.Linear(hidden_dim, hidden_dim)\n",
        "    self.C_r = nn.Linear(2 * hidden_dim ,hidden_dim)\n",
        "\n",
        "    self.W_s = nn.Linear(2 * hidden_dim, hidden_dim)\n",
        "\n",
        "    self.W_a = nn.Linear(hidden_dim, att_dim)\n",
        "    self.U_a = nn.Linear(2 * hidden_dim, att_dim)\n",
        "    self.v_a = nn.Linear(att_dim, 1)\n",
        "\n",
        "    self.U_o = nn.Linear(hidden_dim, hidden_dim)\n",
        "    self.V_o = nn.Linear(embed_dim, hidden_dim)\n",
        "    self.C_o = nn.Linear(2 * hidden_dim, hidden_dim)\n",
        "\n",
        "    self.W_o = nn.Linear((hidden_dim // 2), vocab_size)\n",
        "\n",
        "    self.max_out = nn.MaxPool1d(kernel_size = 2, stride = 2)   # for single  maxout hidden layer\n",
        "\n",
        "  def forward(self, enc_hidden, trg, teacher_forcing_ratio = 0.5):\n",
        "    \"\"\"\n",
        "    enc_hidden : encoder's hidden states , shape of (batch_size, src_length, 2 * hidden_dim)\n",
        "    trg : target sentences, shape of (max_seq_length, batch_size)\n",
        "    \"\"\"\n",
        "    max_seq_length = trg.size(0)\n",
        "    batch_size = enc_hidden.size(0)\n",
        "    src_length = enc_hidden.size(1)\n",
        "    attentions = torch.zeros(max_seq_length, batch_size, src_length)\n",
        "    outputs = torch.zeros(max_seq_length, batch_size, self.vocab_size).to(device)\n",
        "    \n",
        "    s_prev = torch.tanh(self.W_s(enc_hidden[:, -1, :]))   # (batch_size, hidden_dim)\n",
        "    y_prev = self.embedding(trg[0])   # (batch_size, embed_dim)\n",
        "    for i in range(1, max_seq_length):\n",
        "      att = torch.tanh(self.W_a(s_prev).unsqueeze(1).repeat(1, src_length, 1) + self.U_a(enc_hidden))   # (batch_size, src_length, att_dim)\n",
        "      energy = self.v_a(att).squeeze(-1)   # (batch_size, src_length)\n",
        "      attention_weights = (torch.exp(energy) / torch.sum(torch.exp(energy), dim = -1, keepdim = True)).unsqueeze(1)   # (batch_size, 1, src_length)\n",
        "      c = torch.bmm(attention_weights, enc_hidden).squeeze(1)   # (batch_size, 2 * hidden_dim)\n",
        "      z = torch.sigmoid(self.W_z(y_prev) + self.U_z(s_prev) + self.C_z(c))\n",
        "      r = torch.sigmoid(self.W_r(y_prev) + self.U_r(s_prev) + self.C_r(c))\n",
        "      s_ = torch.tanh(self.W(y_prev) + self.U(r * s_prev) + self.C(c))\n",
        "\n",
        "      s = (1 - z) * s_prev + z * s_   # (batch_size, hidden_dim)\n",
        "\n",
        "      out = self.U_o(s_prev) + self.V_o(y_prev) + self.C_o(c)\n",
        "      out = self.max_out(out)   # (batch_size, hidden_dim / 2)\n",
        "      out = self.W_o(out)   # (batch_size, vocab_size)\n",
        "      _, output = torch.max(out, dim = -1)\n",
        "      outputs[i] = out\n",
        "      attentions[i] = attention_weights.squeeze(1)\n",
        "\n",
        "      s_prev = s\n",
        "      # when we train the model, use ground-truth with probability of teacher_forcing_ratio\n",
        "      y_prev = self.embedding(output) if random.random() < teacher_forcing_ratio else self.embedding(trg[i])\n",
        "    \n",
        "    return outputs, attentions\n"
      ],
      "metadata": {
        "id": "dG4tdAFoqXOq"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = Encoder(len(SRC.vocab), embed_dim, hidden_dim).to(device)\n",
        "decoder = Decoder(len(TRG.vocab), embed_dim, hidden_dim, att_dim).to(device)"
      ],
      "metadata": {
        "id": "c2TUCAc_PVxA"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**모델 학습**"
      ],
      "metadata": {
        "id": "Uf9Bf-23Umnw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 논문의 내용대로 정규분포로 초기화\n",
        "def init_weights(model):\n",
        "  for name, param in model.named_parameters():\n",
        "    nn.init.normal_(param.data, mean = 0, std = 0.001)\n",
        "\n",
        "encoder.apply(init_weights) \n",
        "decoder.apply(init_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CtWnX24Pg9fZ",
        "outputId": "65a96222-dc1e-4e5d-a729-207a60734070"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Decoder(\n",
              "  (embedding): Embedding(5893, 620)\n",
              "  (W): Linear(in_features=620, out_features=1000, bias=True)\n",
              "  (U): Linear(in_features=1000, out_features=1000, bias=True)\n",
              "  (C): Linear(in_features=2000, out_features=1000, bias=True)\n",
              "  (W_z): Linear(in_features=620, out_features=1000, bias=True)\n",
              "  (U_z): Linear(in_features=1000, out_features=1000, bias=True)\n",
              "  (C_z): Linear(in_features=2000, out_features=1000, bias=True)\n",
              "  (W_r): Linear(in_features=620, out_features=1000, bias=True)\n",
              "  (U_r): Linear(in_features=1000, out_features=1000, bias=True)\n",
              "  (C_r): Linear(in_features=2000, out_features=1000, bias=True)\n",
              "  (W_s): Linear(in_features=2000, out_features=1000, bias=True)\n",
              "  (W_a): Linear(in_features=1000, out_features=1000, bias=True)\n",
              "  (U_a): Linear(in_features=2000, out_features=1000, bias=True)\n",
              "  (v_a): Linear(in_features=1000, out_features=1, bias=True)\n",
              "  (U_o): Linear(in_features=1000, out_features=1000, bias=True)\n",
              "  (V_o): Linear(in_features=620, out_features=1000, bias=True)\n",
              "  (C_o): Linear(in_features=2000, out_features=1000, bias=True)\n",
              "  (W_o): Linear(in_features=500, out_features=5893, bias=True)\n",
              "  (max_out): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss and optimizer\n",
        "PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = PAD_IDX)\n",
        "params = list(encoder.parameters()) + list(decoder.parameters())\n",
        "optimizer = torch.optim.Adam(params, lr = learning_rate)"
      ],
      "metadata": {
        "id": "Nfi36JJsYEQv"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For checking training time for one epoch\n",
        "def cal_epoch_time(start_time, end_time):\n",
        "  int_time = end_time - start_time\n",
        "  mins = int(int_time / 60)\n",
        "  secs = int(int_time - (mins * 60))\n",
        "  return mins, secs"
      ],
      "metadata": {
        "id": "8cEY6Gq9lUN_"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clip = 1\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  start_time = time.time()   # 시작 시간 기록\n",
        "  for i, batch in enumerate(trainloader):\n",
        "    src = batch.src.to(device)\n",
        "    trg = batch.trg.to(device)\n",
        "    \n",
        "    enc_hidden = encoder(src)\n",
        "    outputs, attentions = decoder(enc_hidden, trg)\n",
        "    \n",
        "    # <sos> 토큰은 학습에 사용하지 않음\n",
        "    voc_size = outputs.size(-1)\n",
        "    outputs = outputs[1:].view(-1, voc_size)\n",
        "    trg = trg[1:].view(-1)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    loss = criterion(outputs, trg)\n",
        "    loss.backward()\n",
        "\n",
        "    # gradient clipping\n",
        "    torch.nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n",
        "    torch.nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    if (i + 1) % 100 == 0:\n",
        "      print(f\"Epoch [{epoch + 1} / {num_epochs}], Step [{i + 1} / {len(trainloader)}], Loss : {loss.item():.3f}, PPL : {math.exp(loss.item()):.3f}\")\n",
        "    if loss < best_valid_loss:\n",
        "      best_valid_loss = loss\n",
        "      torch.save(encoder.state_dict(), \"Attention_NMT_enc.ckpt\")\n",
        "      torch.save(decoder.state_dict(), \"Attention_NMT_dec.ckpt\")\n",
        "  end_time= time.time() # 종료 시간 기록\n",
        "  mins, secs = cal_epoch_time(start_time, end_time)\n",
        "  print(f\"Epoch [{epoch + 1} / {num_epochs}] | Time : {mins}m {secs}s\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OXu3TCDfUPLY",
        "outputId": "d84307e4-42ed-4dc2-c093-b8402152abfc"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1 / 10], Step [100 / 227], Loss : 4.700, PPL : 109.898\n",
            "Epoch [1 / 10], Step [200 / 227], Loss : 4.518, PPL : 91.646\n",
            "Epoch [1 / 10] | Time : 2m 47s\n",
            "Epoch [2 / 10], Step [100 / 227], Loss : 3.856, PPL : 47.269\n",
            "Epoch [2 / 10], Step [200 / 227], Loss : 3.655, PPL : 38.681\n",
            "Epoch [2 / 10] | Time : 2m 11s\n",
            "Epoch [3 / 10], Step [100 / 227], Loss : 3.478, PPL : 32.387\n",
            "Epoch [3 / 10], Step [200 / 227], Loss : 3.057, PPL : 21.259\n",
            "Epoch [3 / 10] | Time : 2m 5s\n",
            "Epoch [4 / 10], Step [100 / 227], Loss : 2.865, PPL : 17.551\n",
            "Epoch [4 / 10], Step [200 / 227], Loss : 2.984, PPL : 19.773\n",
            "Epoch [4 / 10] | Time : 2m 5s\n",
            "Epoch [5 / 10], Step [100 / 227], Loss : 2.594, PPL : 13.383\n",
            "Epoch [5 / 10], Step [200 / 227], Loss : 2.537, PPL : 12.642\n",
            "Epoch [5 / 10] | Time : 2m 3s\n",
            "Epoch [6 / 10], Step [100 / 227], Loss : 2.394, PPL : 10.960\n",
            "Epoch [6 / 10], Step [200 / 227], Loss : 2.039, PPL : 7.680\n",
            "Epoch [6 / 10] | Time : 2m 1s\n",
            "Epoch [7 / 10], Step [100 / 227], Loss : 2.083, PPL : 8.027\n",
            "Epoch [7 / 10], Step [200 / 227], Loss : 1.868, PPL : 6.472\n",
            "Epoch [7 / 10] | Time : 2m 4s\n",
            "Epoch [8 / 10], Step [100 / 227], Loss : 1.687, PPL : 5.404\n",
            "Epoch [8 / 10], Step [200 / 227], Loss : 1.649, PPL : 5.200\n",
            "Epoch [8 / 10] | Time : 2m 1s\n",
            "Epoch [9 / 10], Step [100 / 227], Loss : 1.613, PPL : 5.017\n",
            "Epoch [9 / 10], Step [200 / 227], Loss : 1.370, PPL : 3.934\n",
            "Epoch [9 / 10] | Time : 2m 2s\n",
            "Epoch [10 / 10], Step [100 / 227], Loss : 1.196, PPL : 3.308\n",
            "Epoch [10 / 10], Step [200 / 227], Loss : 2.093, PPL : 8.106\n",
            "Epoch [10 / 10] | Time : 2m 0s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습된 모델 저장\n",
        "from google.colab import files\n",
        "\n",
        "files.download(\"Attention_NMT_dec.ckpt\")\n",
        "files.download(\"Attention_NMT_enc.ckpt\")"
      ],
      "metadata": {
        "id": "odbjwugiqIlg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "encoder.eval()\n",
        "decoder.eval()\n",
        "epoch_loss = 0\n",
        "with torch.no_grad():\n",
        "  for batch in testloader:\n",
        "    src = batch.src.to(device)\n",
        "    trg = batch.trg.to(device)\n",
        "\n",
        "    enc_hidden = encoder(src)\n",
        "    outputs, attentions = decoder(enc_hidden, trg)\n",
        "\n",
        "    voc_size = outputs.size(-1)\n",
        "    outputs = outputs[1:].view(-1, voc_size)\n",
        "    trg = trg[1:].view(-1)\n",
        "\n",
        "    loss = criterion(outputs, trg)\n",
        "    epoch_loss += loss\n",
        "\n",
        "loss = epoch_loss / len(testloader)\n",
        "print(f\"Test Loss : {loss:.3f}, Test PPL : {math.exp(loss):.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wa1g2hbmF21z",
        "outputId": "a8690b08-253d-4f63-b92d-df17a4d805f7"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss : 2.465, Test PPL : 11.765\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**모델 실제로 사용**"
      ],
      "metadata": {
        "id": "Bq16tLX1KGKF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def translate_attention(src, trg):\n",
        "  src_list = [SRC.vocab.itos[word] for word in src]\n",
        "  src_sentence = \" \".join(src_list)\n",
        "  print(f\"소스 문장 : {src_sentence}\")\n",
        "  \n",
        "  trg_list = [TRG.vocab.itos[word] for word in trg]\n",
        "  trg_sentence = \" \".join(trg_list)\n",
        "  print(f\"타겟 문장 : {trg_sentence}\")\n",
        "  \n",
        "  encoder.eval()\n",
        "  decoder.eval()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    enc_hidden = encoder(src)\n",
        "    outputs, attentions = decoder(enc_hidden, trg)\n",
        "\n",
        "  _, out = torch.max(outputs, dim = -1)\n",
        "  translated_sentence = \" \".join([TRG.vocab.itos[word] for word in out])\n",
        "  print(f\"번역된 문장 : {translated_sentence}\")\n",
        "\n",
        "  # ---------------------------------\n",
        "  # for visualizing attention weights\n",
        "  attentions = attentions.squeeze(1).numpy()\n",
        "  attentions = attentions[:len(trg_list), :len(src_list)]\n",
        "\n",
        "\n",
        "  fig, ax = plt.subplots()\n",
        "  im = ax.imshow(attentions)\n",
        "\n",
        "  # Show all ticks and label them with the respective list entries\n",
        "  ax.set_xticks(np.arange(len(trg_list)), trg_list)\n",
        "  ax.set_yticks(np.arange(len(src_list)), src_list) \n",
        "\n",
        "  # Rotate the tick labels and set their alignment\n",
        "  plt.setp(ax.get_xticklabels(), rotation = 45, ha = \"right\", rotation_mode = \"anchor\")\n",
        "\n",
        "  ax.set_title(\"Attention Map\")\n",
        "  fig.tight_layout()\n",
        "  plt.show()\n"
      ],
      "metadata": {
        "id": "lo3hHZuUInsk"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idx = 24\n",
        "\n",
        "for batch in validloader:\n",
        "  src = batch.src[:, idx].unsqueeze(-1)\n",
        "  trg = batch.trg[:, idx].unsqueeze(-1)\n",
        "  break\n",
        "\n",
        "translate_attention(src, trg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        },
        "id": "7GOMaQSfLV55",
        "outputId": "98eecd49-ec10-4be4-8e15-0a399cefdd50"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "소스 문장 : <sos> . hinab häuser viele auf blicken frauen zwei <eos> <pad>\n",
            "타겟 문장 : <sos> two women look out at many houses below . <eos> <pad> <pad>\n",
            "번역된 문장 : <unk> two women are on at many houses . . <eos> <eos> .\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:34: MatplotlibDeprecationWarning: Passing the minor parameter of set_xticks() positionally is deprecated since Matplotlib 3.2; the parameter will become keyword-only two minor releases later.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:35: MatplotlibDeprecationWarning: Passing the minor parameter of set_yticks() positionally is deprecated since Matplotlib 3.2; the parameter will become keyword-only two minor releases later.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "o1P4Pra8MTjr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}